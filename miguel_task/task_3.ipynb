{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34c8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fd15d",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "This notebook implements Task 3 of the ManualsGraph assignment: taking extracted constraints (assumed done in Task 2) and grounding them in a minimal graph schema that supports a neuro-symbolic verifier for proposed LLM actions.\n",
    "\n",
    "The goal is not to build a full pipeline or production database. Instead, I’m demonstrating the smallest working loop that can serve as a guardrail:\n",
    "\n",
    "Manual → Components → Constraints → Verify action → ALLOWED / BLOCKED / REWRITE\n",
    "\n",
    "I use a robot arm datasheet example (Kawasaki KJ series) and focus on torque limits because they are numeric, safety-relevant, and easy to represent as inequalities.\n",
    "\n",
    "* Minimal graph schema (semantic prior)\n",
    "* Neuro-symbolic verification logic (guardrails)\n",
    "\n",
    "## Graph outline:\n",
    "\n",
    "I represent manual knowledge with just three \n",
    "\n",
    "### node types:\n",
    "\n",
    "* `Manual`: the source document and scope\n",
    "\n",
    "* `Component`: the machine part the constraint applies to (e.g., joint JT4)\n",
    "\n",
    "* `Constraint`: a structured limit with a property, operator, value, unit, and provenance\n",
    "\n",
    "### Edges encode the relationships:\n",
    "\n",
    "`Manual -[DESCRIBES]-> Machine (document governance / scope)`\n",
    "\n",
    "`Machine -[HAS_COMPONENT]-> Component`\n",
    "\n",
    "`Constraint -[LIMITS_PARAMETER]-> Component`\n",
    "\n",
    "`Constraint -[EVIDENCED_BY]-> Sentence (optional provenance node)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d92e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "\n",
    "# 1. Manual & Machine (Document Governance Layer)\n",
    "G.add_node(\"manual_robot_arm\", label=\"Manual\", title=\"Kawasaki Robot KJ Series Spec\", version=\"3.0\")\n",
    "G.add_node(\"kj314j\", label=\"Machine\", name=\"KJ314J Robot Arm\", model=\"KJ314J\")\n",
    "G.add_node(\"kj264j\", label=\"Machine\", name=\"KJ264J Robot Arm\", model=\"KJ264J\")\n",
    "\n",
    "G.add_edge(\"manual_robot_arm\", \"kj314j\", type=\"DESCRIBES\")\n",
    "G.add_edge(\"manual_robot_arm\", \"kj264j\", type=\"DESCRIBES\")\n",
    "\n",
    "# 2. Extract Data Loop (Mapping extracted JSON to Schema)\n",
    "extracted_constraints = [\n",
    "    {\"entity\": \"KJ314J_JT4\", \"val\": 56.2, \"src\": \"JT4 56.2 N･m 2.19 kg･m2\"},\n",
    "    {\"entity\": \"KJ314J_JT5\", \"val\": 43.4, \"src\": \"JT5 43.4 N･m 1.31 kg･m2\"},\n",
    "    {\"entity\": \"KJ314J_JT6\", \"val\": 22.0, \"src\": \"JT6 22.0 N･m 0.33 kg･m2\"},\n",
    "    {\"entity\": \"KJ264J_JT4\", \"val\": 56.2, \"src\": \"JT4 56.2 N･m 2.19 kg･m2\"},\n",
    "    {\"entity\": \"KJ264J_JT5\", \"val\": 43.4, \"src\": \"JT5 43.4 N･m 1.31 kg･m2\"},\n",
    "    {\"entity\": \"KJ264J_JT6\", \"val\": 22.0, \"src\": \"JT6 22.0 N･m 0.33 kg･m2\"}\n",
    "]\n",
    "\n",
    "for item in extracted_constraints:\n",
    "    model_id, jt_id = item[\"entity\"].split(\"_\")\n",
    "    node_id = item[\"entity\"].lower()\n",
    "    comp_id = f\"comp_{node_id}\"\n",
    "    const_id = f\"c_{node_id}_torque\"\n",
    "    sent_id = f\"s_{node_id}_spec\"\n",
    "\n",
    "    # Component Node (Axis)\n",
    "    G.add_node(comp_id, label=\"Component\", name=jt_id, component_type=\"axis\")\n",
    "    G.add_edge(model_id.lower(), comp_id, type=\"HAS_COMPONENT\")\n",
    "\n",
    "    # Sentence Node (Provenance)\n",
    "    G.add_node(sent_id, label=\"Sentence\", text=item[\"src\"])\n",
    "\n",
    "    # Constraint Node (Parameter Limit)\n",
    "    G.add_node(const_id, label=\"Constraint\", \n",
    "               property=\"max_torque\", op=\"<=\", value=item[\"val\"], unit=\"Nm\")\n",
    "    \n",
    "    # Evidence & Application Edges\n",
    "    G.add_edge(const_id, comp_id, type=\"LIMITS_PARAMETER\")\n",
    "    G.add_edge(const_id, sent_id, type=\"EVIDENCED_BY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b632273",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_re = re.compile(\n",
    "    r\"^\\s*set\\s+(?P<entity>[a-zA-Z\\d_\\s]+)\\s+\"\n",
    "    r\"(?P<prop>speed|torque)\\s+to\\s+\"\n",
    "    r\"(?P<val>\\d+(?:\\.\\d+)?)\\s*\"\n",
    "    r\"(?P<unit>[a-zA-Z°/]+)\\s*$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def normalize_entity(entity_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Standardizes input strings to match the Graph Node IDs.\n",
    "    Examples:\n",
    "    'KJ314J_JT4' -> 'comp_kj314j_jt4'\n",
    "    'axis_1'     -> 'axis_1' (for the generic machine)\n",
    "    \"\"\"\n",
    "    clean = entity_str.strip().lower().replace(\" \", \"_\")\n",
    "    # Check if it already looks like a component node in our graph\n",
    "    if f\"comp_{clean}\" in G:\n",
    "        return f\"comp_{clean}\"\n",
    "    return clean # Fallback to literal (like 'axis_1')\n",
    "\n",
    "def get_constraints_for(entity_node: str, prop: str):\n",
    "    out = []\n",
    "    if entity_node not in G:\n",
    "        return out\n",
    "\n",
    "    # Search for constraints connected to this component via LIMITS_PARAMETER\n",
    "    # In your graph: Constraint -> LIMITS_PARAMETER -> Component\n",
    "    for u, v, edata in G.in_edges(entity_node, data=True):\n",
    "        if edata.get(\"type\") == \"LIMITS_PARAMETER\":\n",
    "            c = G.nodes[u]\n",
    "            # Normalize property name ('max_torque' vs 'torque')\n",
    "            if c.get(\"label\") == \"Constraint\" and prop in c.get(\"property\", \"\"):\n",
    "                out.append((u, c))\n",
    "    \n",
    "    # Generic mapping for 'axis_1' style nodes (APPLIES_TO)\n",
    "    for u, v, edata in G.in_edges(entity_node, data=True):\n",
    "        if edata.get(\"type\") == \"APPLIES_TO\":\n",
    "            c = G.nodes[u]\n",
    "            if prop in c.get(\"property\", \"\"):\n",
    "                out.append((u, c))\n",
    "\n",
    "    return out\n",
    "\n",
    "def verify(command: str):\n",
    "    m = cmd_re.match(command)\n",
    "    if not m:\n",
    "        return {\"decision\": \"REWRITE\", \"reason\": \"Invalid syntax. Format: Set [entity] [prop] to [val] [unit]\"}\n",
    "\n",
    "    raw_entity = m.group(\"entity\")\n",
    "    entity_node = normalize_entity(raw_entity)\n",
    "    prop = m.group(\"prop\").lower()\n",
    "    val = float(m.group(\"val\"))\n",
    "    unit = m.group(\"unit\").strip()\n",
    "\n",
    "    if entity_node not in G:\n",
    "        return {\"decision\": \"REWRITE\", \"reason\": f\"Unknown machine component: {raw_entity}\"}\n",
    "\n",
    "    constraints = get_constraints_for(entity_node, prop)\n",
    "    if not constraints:\n",
    "        return {\"decision\": \"REWRITE\", \"reason\": f\"No safety limits found for {raw_entity} {prop}\"}\n",
    "\n",
    "    for c_id, c in constraints:\n",
    "        # Check units\n",
    "        c_unit = c.get(\"unit\", \"Nm\")\n",
    "        if c_unit.lower() != unit.lower():\n",
    "            return {\"decision\": \"REWRITE\", \"reason\": f\"Unit mismatch: Manual specifies {c_unit}\"}\n",
    "\n",
    "        # Inequality check\n",
    "        limit = float(c[\"value\"])\n",
    "        if c.get(\"op\") == \"<=\" and val > limit:\n",
    "            return {\n",
    "                \"decision\": \"BLOCKED\",\n",
    "                \"reason\": f\"Value {val} exceeds manual limit of {limit} {c_unit}.\",\n",
    "                \"source\": c.get(\"source_sentence\"),\n",
    "                \"rewrite\": f\"Set {raw_entity} {prop} to {limit} {c_unit}\"\n",
    "            }\n",
    "\n",
    "    return {\"decision\": \"ALLOWED\", \"reason\": \"Command within safe operating parameters.\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4a0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: Set KJ314J_JT4 torque to 100 Nm\n",
      "OUT: BLOCKED - Value 100.0 exceeds manual limit of 56.2 Nm.\n",
      "\n",
      "CMD: Set KJ314J_JT4 torque to 8 Nm\n",
      "OUT: ALLOWED - Command within safe operating parameters.\n",
      "\n",
      "CMD: Set axis_1 torque to 40 Nm\n",
      "OUT: REWRITE - Unknown machine component: axis_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tests = [\n",
    "    \"Set KJ314J_JT4 torque to 100 Nm\",  # Should be BLOCKED (Limit is 56.2)\n",
    "    \"Set KJ314J_JT4 torque to 8 Nm\",       # Should be ALLOWED (Limit is 30)\n",
    "    \"Set axis_1 torque to 40 Nm\",      # Should be Rewrite as axis is unknown\n",
    "\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    result = verify(t)\n",
    "    print(f\"CMD: {t}\")\n",
    "    print(f\"OUT: {result['decision']} - {result['reason']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d921468",
   "metadata": {},
   "source": [
    "The code above is a small prototype that demonstrates the intended loop: parse a command, retrieve the linked constraint from the graph, and check it as an inequality to return ALLOWED, BLOCKED, or REWRITE. In a full system, the parsing and retrieval would be replaced by a DSPy Graph RAG pipeline with a self validation loop, like here: [My made GraphRAG](https://github.com/Str3am786/Scalable_Sys_Project_2/blob/main/src/scalable_sys/rag/graph_rag.py), so the model is forced to produce a consistent structured command and the system can verify that the entity exists, the property matches the manual, and the retrieved constraint actually applies before taking action. Before any comparison, the pipeline should standardize units and scale by mapping unit variants to a canonical form and converting values into a single reference unit per property, using a small conversion dictionary, so the verifier always compares like with like. The symbolic checker remains deterministic and auditable because every decision is grounded in stored constraint nodes that keep provenance back to the source sentence or table row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980956d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
