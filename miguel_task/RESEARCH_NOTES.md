
# Critique: What worked? What was brittle?

What worked: 

Versioning and documentation was strong enough to run and create a start basis. Although, there is an issue with query with the download of PDF's, it seems to have a hard time downloading Robot specification sheets, normally ending up in engine/car specs or even picking up non-manufacturing related documents such as: "Australian Bureau of Statistics" which included census papers; I even got an US department of justice document. A thorough revision of the query used should possibly reduce this unwanted downloads and reduce the need/cost of calling upon AI agents to sort them. Then within sorting, it ran well allowing me to focus on those pdfs which were digital / non scanned. Although, the sole reliance on pypdf may provide difficulties in the future, this is as while tackling this challenge I found that most manufacturing specifications are not within sentences rather tables, which pypdf does not handle with enough precision to create a strong enough dataset. This will need to be changed. Also there could be a few abstractions and breaking down of steps within the scripts into functions to allow reuse of essential parts of code.

For my side of things, as its a prototype/mockup there are various gaps in what I have created. The pdf extraction is no where near good enough and will most likely break with other pdfs though provides a good start especially the Regex part which proves with a correct intial search I can start with a down up approach, where I find the Units we are looking for, then ensure its a constraint, then verify what it relates to. This is as the document might speak of multiple models or just be a single manual for a single model. Examples of these pdfs have been provided within `test_pdfs`.


# Thesis Proposal


This thesis is a complex task tackling both reliable data extraction from pdf's which vary wildly, as well as a reliable constraint extraction and normalisation pipeline. To be used to stop AI's from hallucinating implausible acts. The knowledge base that my verification and RAG system relies on must be trustworthy, with strong guarantees about what it contains and where it came from. That means investing real effort into a database design that tracks manual versions, machine model numbers, and duplicates in a precise, informative, and repeatable way. My Symbolic verification and FactoryBench cannot be built on noisy or incomplete data, because even small errors can corrupt the ground truth and turn it into something unsafe. One incorrect document or constraint can undermine the entire system.

The goal is not to parse every manual perfectly. The goal is to extract a clearly defined set of constraints with high precision, achieve strong coverage on a defined manual corpus, and maintain full traceability so every constraint is auditable and safe to use. This matters because a single wrong manual, wrong revision, or wrong model can silently poison the knowledge base and make downstream verification unreliable.

The overall timeline of the project should be firstly, a solid corpus where we can prove the precision of the extracted information of the pdfs for this I aim to reach a precision of 1.0 on the accepted set with abstention for anything uncertain. This can be done with enough manually revised pdf's to where all styles and patterns are met, I could possibly try tackling a single manufacturer, and ensure my system never fails on their manuals, and slowly concatenate other brands from there. Then move on to a corpus to prove the extraction of our RAG from the created graph and the first examples of the NeuroSymbolic verifier. This way we have a Trustworthy database/dataset as well as a functional enough RAG which the FactoryBench and other projects can start using. This results first approach ensures no one is held back waiting on my project and will allow me to dedicate fully to the extensions necessary to enhance the recall as well as future proof our solution against manufacturer changes and "noise".

There are many ideas and extensions that I can add to this project such as a document governance layer that treats manuals like controlled data assets. Each manual would receive a stable identity derived from vendor, product family, model, revision, language, and publication date. I would store checksums, source location, and parsing provenance, and I would detect near duplicate documents and likely revisions. Or extensions to the parsing stage where I would not treat sentence level text processing as the main problem, because in most of these industrial manuals the constraints that matter are primarily encoded in specification tables and parameter charts rather than in text. That means the thesis should treat table extraction fidelity as a first class objective. Creating a multistage extraction process, leveraging and comparing python pdf extraction packages, regex utilisation with context extraction for an LLM secondary layer. There would also need to be the creation of a normalisation layer where I would ensure that all measurements are of the same unit and type. Additionally a versioning and change detection framework, where we can update manuals depending on robot revisions/manufacturer updates. 

This should be around the 6 month mark considering the manual creation of the corpus the document traceability framework and robust verification of my extraction before even getting started with the RAG and NeuroSymbolic aspect of things.

# How does it connect to FactoryBench?

ManualsGraph can be used in training or even the evaluation of a Model. In a world model setting the model predicts future states and proposes actions, but factories are safety critical and full of hard limits that should not be learned from scratch or rediscovered through trial and error. ManualsGraph provides a semantic prior that encodes those limits directly, and FactoryBench can then use that prior to test whether a world model respects reality under distribution shift, missing sensors, and novel tasks. ManualGraph should be a microservice that FacotryBench calls during evaluation so it can evaluate a candidate action or planned trajectory, my service would check against the constraint graph for the specific machine we are currently assessing and should return the NeuroSymbolic verification: "Allowed", "Blocked", "Rewrite". This will allow the model within FactoryBench to continue or reevaluate what it is doing; meanwhile FactoryBench can assess these failures/successes. This system is hard to "cheat" and creates a solid ground for FactoryBench to evaluate the model.
This integration also enables stronger definitions of generalization. A model that truly generalizes should be able to adapt its behavior when the manual changes, when a machine revision updates limits, or when different vendors use different units and naming conventions. FactoryBench can measure this by holding out manuals, models, or revisions and evaluating whether the world model can still operate safely and efficiently when it is grounded in a new constraint set. It also lets the benchmark separate two failure modes that matter in practice: failures of prediction and failures of safety compliance. Even if the world model predicts dynamics well, it should be penalized if it proposes actions that violate constraints. Conversely, a model that abstains appropriately when constraints or observations are ambiguous can be rewarded for safe behavior. In that sense, ManualsGraph supplies the rules of the game and FactoryBench measures whether a world model can play the game across changing machines and conditions, which is exactly what “generalization” should mean in manufacturing.



